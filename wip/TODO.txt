

Next:
   - Output files
      Check output files
      Can we disable compiler passes, e.g. constant folding? How are passes decided?
      What part of constant folding performs our change, print it
   - Look at a pass, and how to write your own pass and add it?
   - How to take HLO proto and run our program?
   - Creatig your own backend
   - Additional flags throughout the entire thing
   - GPUs and other devices and copying memory etc
   - SPMD and other distribution mechanisms


==============================================================================
Shape Tutorial
// tf/compiler/xla/shape_utils.h is great
// Creates an opaque shape. These are generally used for threading a context into a custom operation.
static Shape MakeOpaqueShape();

// Creates a token shape. Values of this shape are used for ordering
// side-effecting operations.
static Shape MakeTokenShape();

==============================================================================
Adding things to t05_first_hlo_program
exeutableBuildOptions.set_device_allocator();
executableBuildOptions.mutable_debug_options()->set_xla_detailed_logging_and_dumping(options.detailed_logging);


==============================================================================
StreamExecutor Documentation
  // StreamExecutor manages a single device, in terms of executing work 
  //   class StreamExecutor (tf/stream_executor/stream_executor_pimpl.h)
  //     port::Status Init(DeviceOptions device_options);
  //
  // Base class
  //     DeviceMemoryAllocator [Abstract] (tf/se/device_memory_allocator.h)
  //
  // Implementing classes
  //
  //   1a. StreamExecutorMemoryAllocator : DeviceMemoryAllocator (tf/se/device_memory_allocator.h)
  //      Default memory allocator for a platform which use StreamExecutor::Allocate/Deallocate
  //      StreamExecutorMemoryAllocator(StreamExecutor*)
  //
  //    b. StreamExecutor (tf/se/stream_executor_pimpl.h)
  //      Manages execution of work on a single device, in terms of executing work (kernel launches and memory management 
  //      Takes a StreamExecutorInterface at construction as implementation (e.g. if it's a CUDA or OpenCL executor)
  //      - StreamExecutor(const Platform*, std::unique_ptr<internal::StreamExecutorInterface>, int device_ordinal);
  //      OBS these methods will proxy to implementation class
  //      - port::Status Init()  // Uses DeviceOptions::Default()
  //      - port::Status Init(DeviceOptions device_options)
  //
  //    c. StreamExecutorInterface [abstract] (tf/se/stream_executor_internal.h)
  //      Interface for the different StreamExecutor platforms (i.e. CUDA, OpenCL).
  //
  //      1. XlaInterpreterExecutor (tf/compiler/xla/service/interpreter/executor.h)
  //         A CPU-only implementation of the StreamExecutor interface.
  //         Used for testing and to examine the performance of host-based StreamExecutor code.
  //         - XlaInterpreterExecutor(const PluginConfig &);
  //         - port::Status Init(int device_ordinal, DeviceOptions);
  // 
  //      2. GpuExecutor(const PluginConfig&) (tf/se/gpu/gpu_executor.h)
  //         The CUDA implementation of the StreamExecutorInterface functionality.
  //         StreamExecutor basically correspond to the CUDA streams programming model provided by the
  //         libcuda.so driver APIs, so we don't have to do much more than wrap the calls to the libraries appropriately.
  //         - GpuExecutor(const PluginConfig&)
  //         - port::Status Init(int device_ordinal, DeviceOptions);
  //
  //      3. HostExecutor (tf/se/host/host_gpu_executor.h)
  //         A CPU-only implementation of StreamExecutor, that does no communication or interaction with a device.  
  //         Used for testing and to examine the performance of host-based StreamExecutor code.
  //         - HostExecutor (const PluginConfig&);
  //         - port::Status Init(int device_ordinal, DeviceOptions device_options) override;
  //
  //   2. Adapter class that wraps a Tensorflow allocator. 
  //     TfAllocatorAdapter(tf::Allocator *wrapped, Stream *stream) (tf/se/tf_allocator_adapter.h)


==============================================================================
Compiler and Stuff

LocalClient : Client (tf/compiler/xla/client/local_client.h)
LocalExecutable (tf/compiler/xla/client/local_client.h)
ExecutableBuildOptions (tf/compiler/xla/client/executable_build_options.h)

//---
Platform [abstract] (tf/stream_executor/platform.h)
   void*        Id();  // Uniquely identifies this platform
   std::string  Name();
   port::Status Initialize(const std::map<std::string, std::string>& platform_options);
   bool         Initialized() const;
   port::StatusOr<StreamExecutor*> GetExecutor(const StreamExecutorConfig& config);
   void RegisterTraceListener(std::unique_ptr<TraceListener> listener);

//---
HostPlatform : Platform (tf/stream_executor/host/host_platform.h)
   Has always Name: "Host", and a unique id

//---
Compiler [abstract] (tf/compiler/xla/service/compiler.h)
   class CompileOptions {
      se::DeviceMemoryAllocator* device_allocator;
      tf::thread::ThreadPool*    thread_pool;
   };

   Platform::Id PlatformId();

  // Runs Hlo passes to optimize the given Hlo module, returns the optimized module.
   StatusOr<std::unique_ptr<HloModule>> RunHloPasses(
      std::unique_ptr<HloModule> module,
      se::StreamExecutor* executor,
      const CompileOptions& options);

   // Performs scheduling and buffer assignment and returns the buffer assignments.
   // This base class returns error: Unimplemented("This compiler does not support this method");
   StatusOr<std::unique_ptr<BufferAssignment>> AssignBuffers(const HloModule* module);

   StatusOr<std::unique_ptr<Executable>> RunBackend(
      std::unique_ptr<HloModule> module,
      se::StreamExecutor* executor,
      const CompileOptions& options);

   // Compiles a set of HLO modules that can run in parallel, potentially communicating data
   // between the modules, and returns a corresponding sequence of executable objects.
   StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(
      std::unique_ptr<HloModuleGroup> module_group,
      std::vector<std::vector<se::StreamExecutor*>> stream_exec,
      const CompileOptions& options);

  // A CompilerFactory is a function object: std::unique_ptr<Compiler> foo()
  static std::map<se::Platform::Id, CompilerFactory> compiler_factories;
  static std::map<se::Platform::Id, std::unique_ptr<Compiler>> platform_compilers;
  static ... RegisterCompilerFactory(...);    // Map platformId -> CompilerFactory
  static ... GetPlatformCompilerFactories();  // Map platformId -> CompilerFactory
  static ... GetPlatformCompilers();          // Map platformId -> Compiler*

  // Find a compiler either caches in PlatformCompilers, or use factory to create one and cache it
  // If none is cached, and there is no factory for it, "try adding tensorflow/compiler/jit:xla_cpu_jit as deps"
  static StatusOr<Compiler*> Compiler::GetForPlatform(se:Platform*)

//---
LLVMCompiler : Compiler (tf/compiler/xla/service/llvm_compiler.cc)

  // A callback of this type can be run before and/or after IR-level optimization.
  // E.g. to dump out the generated IR to disk or gather some statistics.
  using ModuleHook = std::function<void(const llvm::Module&)>;  // "llvm/IR/Module.h"
  void SetPreOptimizationHook(ModuleHook hook);
  void SetPostOptimizationHook(ModuleHook hook);

  // I can't see much LLVM specfic in this implementation
  StatusOr<std::vector<std::unique_ptr<Executable>>> Compile(
      std::unique_ptr<HloModuleGroup> module_group,
      std::vector<std::vector<se::StreamExecutor*>> stream_execs,
      const CompileOptions& options);

//---
CpuCompiler
   Lots of interesting stuff here involving LLVM

==============================================================================
Stack trace for Compile

LocalClient::Compile
   LocalService::CompileExecutables
      Service::BuildExecutable
         HloModule::CreateFromProto
         HloVerifier::Run
         DumpHloModuleIfEnabled name: "before_optimizations" 941 @ ./tensorflow/compiler/xla/service/dump.cc
         compiler 260 @ ./tensorflow/compiler/xla/service/backend.h
       CpuCompiler::RunHloPasses runs majority ~100k lines of trace code
         CpuCompiler::RunHloPasses 1042 @ ./tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
            config [1] [] [] [ 450 @ ./tensorflow/compiler/xla/service/hlo_module.h]
            CompilerTargetOptions [1] [] [] [ 914 @ ./tensorflow/compiler/xla/service/cpu/cpu_compiler.cc]
            config [1] [] [] [ 450 @ ./tensorflow/compiler/xla/service/hlo_module.h]
            CodeGenOptLevel [1] [] [] [ 924 @ ./tensorflow/compiler/xla/service/cpu/cpu_compiler.cc]
               debug_options [1] [] [] [ 427 @ ./tensorflow/compiler/xla/service/hlo_module_config.h]
            SimpleOrcJIT::InferTargetMachineForJIT [1] [] [] [ 247 @ ./tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc]
          Next line trace ~100k of code
            CpuCompiler::RunHloPasses [1] [] [] [ 885 @ ./tensorflow/compiler/xla/service/cpu/cpu_compiler.cc]
               HloModule::ToProto [1] [] [] [ 492 @ ./tensorflow/compiler/xla/service/hlo_module.cc]
               LLVMTargetMachineFeatures [1] [] [] [ 243 @ ./tensorflow/compiler/xla/service/cpu/target_machine_features.h]
             Next line trace ~100k of code
               CpuCompiler::RunHloPassesThroughLayoutAssn [1] [] [] [ 631 @ ./tensorflow/compiler/xla/service/cpu/cpu_compiler.cc]





==============================================================================
### Trace output when creating TensorFlow operations (from retm1)
DeviceAssignment [1] [] [] [ 210 @ ./tensorflow/compiler/xla/service/computation_placer.h]
RunId::RunId [1] [] [] [ 191 @ ./tensorflow/compiler/xla/executable_run_options.cc]
RunId [1] [] [] [ 224 @ ./tensorflow/compiler/xla/executable_run_options.h]
ExecutableRunOptions::set_run_id [1] [] [] [ 355 @ ./tensorflow/compiler/xla/executable_run_options.cc]
ExecutableRunOptions::set_stream [1] [] [] [ 245 @ ./tensorflow/compiler/xla/executable_run_options.cc]
ExecutableRunOptions::set_allocator [1] [] [] [ 228 @ ./tensorflow/compiler/xla/executable_run_options.cc]
ExecutableRunOptions::set_intra_op_thread_pool [1] [] [] [ 277 @ ./tensorflow/compiler/xla/executable_run_options.cc]
GetXLARandomSeed [1] [] [] [ 951 @ ./tensorflow/compiler/tf2xla/tf2xla_util.cc]
ExecutableRunOptions::set_rng_seed [1] [] [] [ 342 @ ./tensorflow/compiler/xla/executable_run_options.cc]
LocalExecutable::Run [1] [] [] [ 371 @ ./tensorflow/compiler/xla/client/local_client.cc]

ConsumeResult [1] [] [] [ 436 @ ./tensorflow/compiler/xla/service/executable.h]
XlaComputationLaunchContext::PopulateOutputs [1] [] [] [ 671 @ ./tensorflow/compiler/jit/xla_launch_util.cc]

XlaCompilationCache::BuildExecutable [1] [] [] [ 477 @ ./tensorflow/compiler/jit/xla_compilation_cache.cc]
   LocalClient::default_device_ordinal [1] [] [] [ 558 @ ./tensorflow/compiler/xla/client/local_client.cc]
ExecutableBuildOptions::set_device_ordinal [1] [] [] [ 211 @ ./tensorflow/compiler/xla/client/executable_build_options.cc]
ExecutableBuildOptions::set_result_layout [1] [] [] [ 236 @ ./tensorflow/compiler/xla/client/executable_build_options.cc]
ExecutableBuildOptions::set_device_allocator [1] [] [] [ 195 @ ./tensorflow/compiler/xla/client/executable_build_options.cc]
set_alias_passthrough_params [1] [] [] [ 316 @ ./tensorflow/compiler/xla/client/executable_build_options.h]
ExecutableBuildOptions::mutable_debug_options [1] [] [] [ 225 @ ./tensorflow/compiler/xla/client/executable_build_options.cc]
LocalClient::Compile [1] [] [] [ 610 @ ./tensorflow/compiler/xla/client/local_client.cc]

LocalClient::default_device_ordinal [1] [] [] [ 558 @ ./tensorflow/compiler/xla/client/local_client.cc]
is_on_xla_device [1] [] [] [ 250 @ ./tensorflow/compiler/jit/xla_platform_info.h]
executable [1] [] [] [ 247 @ ./tensorflow/compiler/xla/client/local_client.h]
module [1] [] [] [ 621 @ ./tensorflow/compiler/xla/service/executable.h]
input_output_alias_config [1] [] [] [ 542 @ ./tensorflow/compiler/xla/service/hlo_module.h]

// NoOp
SetOpMetadata [1] [] [] [ 379 @ ./tensorflow/compiler/xla/client/xla_builder.h]
frontend_attributes [1] [] [] [ 444 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedFrontendAttributesAssignment [1] [] [] [ 1831 @ ./tensorflow/compiler/xla/client/xla_builder.h]
sharding [1] [] [] [ 465 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedShardingAssignment [1] [] [] [ 1792 @ ./tensorflow/compiler/xla/client/xla_builder.h]
ClearOpMetadata [1] [] [] [ 405 @ ./tensorflow/compiler/xla/client/xla_builder.h]

// Const
SetOpMetadata [1] [] [] [ 379 @ ./tensorflow/compiler/xla/client/xla_builder.h]
frontend_attributes [1] [] [] [ 444 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedFrontendAttributesAssignment [1] [] [] [ 1831 @ ./tensorflow/compiler/xla/client/xla_builder.h]
sharding [1] [] [] [ 465 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedShardingAssignment [1] [] [] [ 1792 @ ./tensorflow/compiler/xla/client/xla_builder.h]
ClearOpMetadata [1] [] [] [ 405 @ ./tensorflow/compiler/xla/client/xla_builder.h]
DfsHloVisitorBase [1] [] [] [ 235 @ ./tensorflow/compiler/xla/service/dfs_hlo_visitor.h]
DfsHloVisitorWithDefaultBase [1] [] [] [ 216 @ ./tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h]
HloEvaluator::HloEvaluator [1] [] [] [ 924 @ ./tensorflow/compiler/xla/service/hlo_evaluator.cc]
ValueInference [1] [] [] [ 258 @ ./tensorflow/compiler/xla/client/value_inference.h]
...
XlaExpression::Constant [1] [] [] [ 206 @ ./tensorflow/compiler/tf2xla/xla_expression.cc]
   XlaOp [1] [] [] [ 247 @ ./tensorflow/compiler/xla/client/xla_builder.h]
...
Tensor::Tensor [1] [] [] [ 1202 @ ./tensorflow/core/framework/tensor.cc]
   Allocate [1] [] [] [ 207 @ ./tensorflow/core/framework/typed_allocator.h]
   AllocateRaw [1] [] [] [ 307 @ ./tensorflow/core/framework/allocator.h]
      AllocateRaw [1] [] [] [ 222 @ ./tensorflow/compiler/tf2xla/xla_compilation_device.cc]
        AlignedMalloc [1] [] [] [ 466 @ ./tensorflow/core/platform/default/port.cc]
           XlaOp [1] [] [] [ 247 @ ./tensorflow/compiler/xla/client/xla_builder.h]
...
ClearOpMetadata [1] [] [] [ 405 @ ./tensorflow/compiler/xla/client/xla_builder.h]

// _RetVal
SetOpMetadata [1] [] [] [ 379 @ ./tensorflow/compiler/xla/client/xla_builder.h]
frontend_attributes [1] [] [] [ 444 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedFrontendAttributesAssignment [1] [] [] [ 1831 @ ./tensorflow/compiler/xla/client/xla_builder.h]
sharding [1] [] [] [ 465 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedShardingAssignment [1] [] [] [ 1792 @ ./tensorflow/compiler/xla/client/xla_builder.h]
ClearOpMetadata [1] [] [] [ 405 @ ./tensorflow/compiler/xla/client/xla_builder.h]
...
DfsHloVisitorBase [1] [] [] [ 235 @ ./tensorflow/compiler/xla/service/dfs_hlo_visitor.h]
DfsHloVisitorWithDefaultBase [1] [] [] [ 216 @ ./tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h]
HloEvaluator::HloEvaluator [1] [] [] [ 924 @ ./tensorflow/compiler/xla/service/hlo_evaluator.cc]
...
ValueInference [1] [] [] [ 258 @ ./tensorflow/compiler/xla/client/value_inference.h]
...
XlaContext::SetRetval [1] [] [] [ 253 @ ./tensorflow/compiler/tf2xla/xla_context.cc]
   XlaOp [1] [] [] [ 247 @ ./tensorflow/compiler/xla/client/xla_builder.h]
   shape [1] [] [] [ 538 @ ./tensorflow/core/framework/tensor.h]
   TensorShapeRep::TensorShapeRep [1] [] [] [ 977 @ ./tensorflow/core/framework/tensor_shape.h]
   Tensor::Tensor [1] [] [] [ 1356 @ ./tensorflow/core/framework/tensor.h]

// NoOp
SetOpMetadata [1] [] [] [ 379 @ ./tensorflow/compiler/xla/client/xla_builder.h]
def [1] [] [] [ 335 @ ./tensorflow/core/framework/op_kernel.h]
frontend_attributes [1] [] [] [ 444 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedFrontendAttributesAssignment [1] [] [] [ 1831 @ ./tensorflow/compiler/xla/client/xla_builder.h]
sharding [1] [] [] [ 465 @ ./tensorflow/compiler/xla/client/xla_builder.h]
XlaScopedShardingAssignment [1] [] [] [ 1792 @ ./tensorflow/compiler/xla/client/xla_builder.h]
ClearOpMetadata [1] [] [] [ 405 @ ./tensorflow/compiler/xla/client/xla_builder.h]
...
XlaComputation [1] [] [] [ 200 @ ./tensorflow/compiler/xla/client/xla_computation.h]


********************************************************
Extracts from BUILDs that may/may not be relevant anymore

# NOTE 1:
# Entries below these lines added to make xla::Compiler::GetForPlatform, and xla::ClientLibrary::GetOrCreateLocalClient
# work. Otherwise their return status is NOT_FOUND for those calls.
# Observe, no code changes are required, just linking these in make these call go to OK instead of NOT_FOUND so
# probably a dependency on a global variable/constructor that registers these.
# Only //tensorflow/compiler/jit:xla_cpu_jit is specified as needed but it needs to subsequent ones to link correctly
# (i.e. no undefined symbol).
# Unfortunate side-effect is that below addition increases link time from 53s to 2572s. 
# Maybe it's possible to figure out how Compiler::GetForPlatform and ClientLibrary::GetOrCreateLocalClient depend
# on //tensorflow/compiler/jit:xla_cpu_jit to shorten this down.
#        "//tensorflow/compiler/jit:xla_cpu_jit",
#        "//tensorflow/core/common_runtime:core_cpu_impl",
#        "//tensorflow/core/common_runtime/gpu:gpu_runtime_impl",
#        "//tensorflow/cc/saved_model:bundle_v2",
#        "//tensorflow/core/grappler/optimizers:custom_graph_optimizer_registry_impl",
#        "//tensorflow/core/profiler/internal/cpu:annotation_stack",



